{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03a140bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, pipeline, GPT2LMHeadModel, GPT2Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c3efd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.tokens import Token, TOKEN_TYPE_EXPRESSIONS, TOKEN_TYPE_ANSWERS\n",
    "from model.equation_interpreter import Equation\n",
    "from model.vocabulary import Vocabulary\n",
    "from model.tokens import Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb53ab99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import disable_caching\n",
    "disable_caching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e87927a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined vocabulary\n",
    "vocabulary = Vocabulary.construct_from_list(TOKEN_TYPE_EXPRESSIONS + TOKEN_TYPE_ANSWERS)\n",
    "\n",
    "# Global variables\n",
    "model_name = \"JustSumAI\"\n",
    "project_name = \"JustSumAI\"\n",
    "repo_name = f\"{model_name}_cleaned_gpt2_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0ddfe40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1212,  318,  616, 5128, 8379,   13]]), torch.Size([1, 6]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "input_text = \"This is my input sequence.\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "input_ids, input_ids.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "502f365a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.end_seq_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937b64a1",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fce02561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f046c2ea5e1444658b70f9bc7be68a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/943 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hugo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Hugo\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac31fc125db4ca4a578d0abe82263ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/356M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a80cad7f970f4683864921fbb40722bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/134 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(68, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=68, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(f\"Dragonoverlord3000/{model_name}\", force_download=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "015df806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 Number of parameters: 85.89M\n"
     ]
    }
   ],
   "source": [
    "print(f\"GPT-2 Number of parameters: {model.num_parameters()/1_000_000:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80bd83fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([64, 40, 39, 19, 19, 67]), torch.Size([1, 6]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_example_ids = torch.LongTensor([vocabulary.vectorize([\"#\", \"/\", \"0\", \"0\"])[:-1] + [vocabulary.separator_index]])\n",
    "test_example_ids[0], test_example_ids.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af631d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 68])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<BEGIN>', '#', '/', '0', '0', 'TT_ZERO']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = model(test_example_ids).logits\n",
    "print(test.size())\n",
    "[vocabulary.getToken(torch.argmax(o).item()) for o in test[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9be1176a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hugo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1288: UserWarning: Using `max_length`'s default (227) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[64, 40, 39, 19, 19, 67, 51, 49, 59, 49, 57, 60, 59, 49, 61, 65]]),\n",
       " torch.Size([1, 16]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.generate(test_example_ids, \n",
    "                     eos_token_id=vocabulary.end_seq_index, \n",
    "                     pad_token_id=vocabulary.mask_index)\n",
    "out, out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acf8ef09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<BEGIN>',\n",
       "  '#',\n",
       "  '/',\n",
       "  '0',\n",
       "  '0',\n",
       "  '[SEP]',\n",
       "  'TT_ZERO',\n",
       "  'TT_INTEGER',\n",
       "  'TT_MINUS',\n",
       "  'TT_INTEGER',\n",
       "  'TT_LOG',\n",
       "  'TT_MULTIPLY',\n",
       "  'TT_MINUS',\n",
       "  'TT_INTEGER',\n",
       "  'TT_DIVIDE',\n",
       "  '<END>'],\n",
       " 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [vocabulary.getToken(o.item()) for o in out[0]]\n",
    "l, l.index(\"[SEP]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f85abbc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[---- Type: TT_ZERO \t Value: None ----,\n",
       " ---- Type: TT_INTEGER \t Value: None ----,\n",
       " ---- Type: TT_MINUS \t Value: None ----,\n",
       " ---- Type: TT_INTEGER \t Value: None ----,\n",
       " ---- Type: TT_LOG \t Value: None ----,\n",
       " ---- Type: TT_MULTIPLY \t Value: None ----,\n",
       " ---- Type: TT_MINUS \t Value: None ----,\n",
       " ---- Type: TT_INTEGER \t Value: None ----,\n",
       " ---- Type: TT_DIVIDE \t Value: None ----]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eq = Equation([Token(vocabulary.getToken(o.item())) for o in out[0]][l.index(\"[SEP]\")+1:-1], notation=\"postfix\")\n",
    "eq.tokenized_equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdf6adad",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43meq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetMathemetaicalNotation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\DTU\\01666\\fagprojekt-01666\\code\\model\\equation_interpreter.py:242\u001b[0m, in \u001b[0;36mEquation.getMathemetaicalNotation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetMathemetaicalNotation\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpostfix\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefix\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 242\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvertToInfix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfix\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    244\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\Documents\\DTU\\01666\\fagprojekt-01666\\code\\model\\equation_interpreter.py:176\u001b[0m, in \u001b[0;36mEquation.convertToInfix\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m         left, new_idx \u001b[38;5;241m=\u001b[39m _postfixToInfix(new_idx)\n\u001b[0;32m    174\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [Token(TT_LEFT_PARENTHESIS)] \u001b[38;5;241m+\u001b[39m left \u001b[38;5;241m+\u001b[39m [token] \u001b[38;5;241m+\u001b[39m right \u001b[38;5;241m+\u001b[39m [Token(TT_RIGHT_PARENTHESIS)], new_idx\n\u001b[1;32m--> 176\u001b[0m infix_token_list, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_postfixToInfix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenized_equation \u001b[38;5;241m=\u001b[39m infix_token_list\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfix\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\Documents\\DTU\\01666\\fagprojekt-01666\\code\\model\\equation_interpreter.py:173\u001b[0m, in \u001b[0;36mEquation.convertToInfix.<locals>._postfixToInfix\u001b[1;34m(idx)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mt_type \u001b[38;5;129;01min\u001b[39;00m BIN_OPERATOR_TYPES:\n\u001b[0;32m    172\u001b[0m     right, new_idx \u001b[38;5;241m=\u001b[39m _postfixToInfix(idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 173\u001b[0m     left, new_idx \u001b[38;5;241m=\u001b[39m \u001b[43m_postfixToInfix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [Token(TT_LEFT_PARENTHESIS)] \u001b[38;5;241m+\u001b[39m left \u001b[38;5;241m+\u001b[39m [token] \u001b[38;5;241m+\u001b[39m right \u001b[38;5;241m+\u001b[39m [Token(TT_RIGHT_PARENTHESIS)], new_idx\n",
      "File \u001b[1;32m~\\Documents\\DTU\\01666\\fagprojekt-01666\\code\\model\\equation_interpreter.py:173\u001b[0m, in \u001b[0;36mEquation.convertToInfix.<locals>._postfixToInfix\u001b[1;34m(idx)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mt_type \u001b[38;5;129;01min\u001b[39;00m BIN_OPERATOR_TYPES:\n\u001b[0;32m    172\u001b[0m     right, new_idx \u001b[38;5;241m=\u001b[39m _postfixToInfix(idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 173\u001b[0m     left, new_idx \u001b[38;5;241m=\u001b[39m \u001b[43m_postfixToInfix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [Token(TT_LEFT_PARENTHESIS)] \u001b[38;5;241m+\u001b[39m left \u001b[38;5;241m+\u001b[39m [token] \u001b[38;5;241m+\u001b[39m right \u001b[38;5;241m+\u001b[39m [Token(TT_RIGHT_PARENTHESIS)], new_idx\n",
      "File \u001b[1;32m~\\Documents\\DTU\\01666\\fagprojekt-01666\\code\\model\\equation_interpreter.py:165\u001b[0m, in \u001b[0;36mEquation.convertToInfix.<locals>._postfixToInfix\u001b[1;34m(idx)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_postfixToInfix\u001b[39m(idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m--> 165\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[43mtoken_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mt_type \u001b[38;5;241m==\u001b[39m TT_INTEGER \u001b[38;5;129;01mor\u001b[39;00m token\u001b[38;5;241m.\u001b[39mt_type \u001b[38;5;241m==\u001b[39m TT_RATIONAL \u001b[38;5;129;01mor\u001b[39;00m token\u001b[38;5;241m.\u001b[39mt_type \u001b[38;5;241m==\u001b[39m TT_VARIABLE \u001b[38;5;129;01mor\u001b[39;00m token\u001b[38;5;241m.\u001b[39mt_type \u001b[38;5;129;01min\u001b[39;00m SPECIAL_NUMBERS_TYPES:\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [token], idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "eq.getMathemetaicalNotation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd63a47",
   "metadata": {},
   "source": [
    "______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478ebbf3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sp.parse_expr(eq.getMathemetaicalNotation(), evaluate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb35ff2c",
   "metadata": {},
   "source": [
    "# Embedding Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea4ce64",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = model.transformer.wte.weight  # Word Token Embeddings \n",
    "position_embeddings = model.transformer.wpe.weight  # Word Position Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b9c301",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings, word_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b84af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "U,D,V = np.linalg.svd(word_embeddings.detach().numpy())\n",
    "plt.xlabel(\"The i'th singular value\")\n",
    "plt.ylabel(r\"Value: $\\sigma^2$\")\n",
    "plt.plot(D, color=\"orange\", linewidth=2)\n",
    "plt.legend([\"Roots in $\\mathbb{Z}$\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cccfe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"Qwordembeds.txt\", \"w\") as f:\n",
    "#     f.write(json.dumps(word_embeddings.detach().numpy().tolist()))\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a975ad",
   "metadata": {},
   "source": [
    "### Accept user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aada31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f196cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator_degree = 3\n",
    "denominator_degree = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2277cb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parentherizer(val):\n",
    "    if \"-\" in val:\n",
    "        return f\"({val})\"\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dead073",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_display = widgets.HTMLMath(\n",
    "    value=f\"Sum Math\",\n",
    "    placeholder='',\n",
    "    description='',\n",
    ")\n",
    "\n",
    "numerator_list = [widgets.SelectionSlider(\n",
    "    options=TOKEN_TYPE_EXPRESSIONS[:-2],\n",
    "    value='-5',\n",
    "    description=f'Root {i+1}',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True\n",
    ") for i in range(numerator_degree)]\n",
    "\n",
    "denominator_list = [\n",
    "    widgets.SelectionSlider(\n",
    "    options=TOKEN_TYPE_EXPRESSIONS[:-2],\n",
    "    value='-5',\n",
    "    description=f'Root {i+1}',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True\n",
    ") for i in range(denominator_degree)]\n",
    "\n",
    "ui_1 = widgets.VBox(numerator_list)\n",
    "ui_2 = widgets.VBox(denominator_list)\n",
    "\n",
    "def f(**kwargs):\n",
    "#     print(kwargs, list(kwargs))\n",
    "    sum_str = r\"$$\\sum_{n=1}^{\\infty}\\frac{\" + \"\".join([f\"(n - {parentherizer(kwargs[str(i)])})\" for i in range(numerator_degree)]) + \"}{\" + \"\".join([f\"(n - {parentherizer(kwargs[str(i + numerator_degree)])})\" for i in range(denominator_degree)]) + \"}\" + \"$$\"\n",
    "    math_display.value = sum_str\n",
    "    display(math_display)\n",
    "\n",
    "out = widgets.interactive(f, **{str(i):v for i,v in enumerate(numerator_list + denominator_list)})\n",
    "\n",
    "display(out)#ui_1, ui_2, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611b05e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "[v.value for v in numerator_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd2929",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_sum = vocabulary.vectorize(\n",
    "    [v.value for v in numerator_list] + [\"/\"] + [v.value for v in denominator_list]\n",
    ")[:-1] + [vocabulary.separator_index]\n",
    "input_sum = torch.LongTensor([input_sum])\n",
    "out = model.generate(input_sum,\n",
    "                    eos_token_id=vocabulary.end_seq_index, \n",
    "                    pad_token_id=vocabulary.mask_index)\n",
    "pred = [vocabulary.getToken(o.item()) for o in out[0]]\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e8b02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eq = Equation([Token(p) for p in pred][pred.index(\"[SEP]\")+1:-1], notation=\"postfix\")\n",
    "eq.getMathmetaicalNotation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e0ba4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b517cf72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2c8093",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
