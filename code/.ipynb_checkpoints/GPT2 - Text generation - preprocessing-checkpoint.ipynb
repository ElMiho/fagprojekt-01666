{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac413024",
   "metadata": {},
   "source": [
    "### Description\n",
    "We model the prediction using a transformer decoder, we train GPT-2 to predict the next token. The trick is, that we let every sentence be of the form:\n",
    "\\begin{align}\n",
    "    &[\"1\",\"5/2\",\"/\",\"0\",\"-1\", \"[SEPARATOR]\", \"<MASK>\"] \\\\ \n",
    "    &--- \\textrm{Predicted that} \"<MASK>\" = \"Z\" ---> \\\\\n",
    "    &[\"1\",\"5/2\",\"/\",\"0\",\"-1\", \"[SEPARATOR]\", \"Z\", \"<MASK>\"]\n",
    "\\end{align}\n",
    "where \"[SEPARATOR]\" indicates that we're not dealing with the polynomial anymore, we're dealing with the 'answer'. This process is continued iteratively untill the max-length is reached or the network predicts an EOS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5fc05b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External imports\n",
    "import torch\n",
    "\n",
    "# For saving and publishing models and datasets to the huggingface hub\n",
    "from huggingface_hub import notebook_login, create_repo\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset, DownloadConfig, IterableDataset\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import linecache\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8db90d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "Your token has been saved in your configured git credential helpers (manager-core).\n",
      "Your token has been saved to C:\\Users\\Hugo\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c2bfd5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domestic imports\n",
    "from model.tokenize_input import input_string_to_tokenize_expression\n",
    "from model.tokens import TOKEN_TYPE_ANSWERS, TOKEN_TYPE_EXPRESSIONS\n",
    "from model.equation_interpreter import Equation\n",
    "from model.vocabulary import Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "07860756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([64, 40, 39, 19, 9, 67, 49, 65],\n",
       " ['<BEGIN>', '#', '/', '0', '-1', '[SEP]', 'TT_INTEGER', '<END>'])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a combined vocabulary\n",
    "vocabulary = Vocabulary.construct_from_list(TOKEN_TYPE_EXPRESSIONS + TOKEN_TYPE_ANSWERS)\n",
    "vectorized_sample = vocabulary.vectorize([\"#\", \"/\", \"0\", \"-1\", \"[SEP]\", \"TT_INTEGER\"])\n",
    "vectorized_sample, [vocabulary.getToken(idx) for idx in vectorized_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6d36a1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"JustSumAI\"\n",
    "repo_name = f\"{model_name}_cleaned_gpt2_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b0ccb3",
   "metadata": {},
   "source": [
    "# Dataset creation\n",
    "Takes `input_file_answers` and `input_file_expressions` and combines every convertable version (i.e. does not contain PolyGamma or similar) of these, separated by a `\"[SEP]\"` token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "88996d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of evaluation datapoints\n",
    "num_eval_points = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5ed87398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to data files\n",
    "input_file_answers = \"./data/answers-1000.txt\"\n",
    "input_file_expressions = \"./data/expressions-1000.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "313c73b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3dc7faad8f546a98506e70a60c28611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [RootSum]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [Zeta]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [RootSum]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [RootSum]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [RootSum]\n",
      "Unknown token encountered: [RootSum]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [RootSum]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [RootSum]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [RootSum]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [RootSum]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [RootSum]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [RootSum]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "Unknown token encountered: [PolyGamma]\n",
      "\n",
      "Stats: \n",
      "--- Initial size: 1000 \n",
      "--- Time since start: 0.1603 s \n",
      "--- Successes: 336 \n",
      "--- Fails: 665 \n",
      "--- Max length: 194\n"
     ]
    }
   ],
   "source": [
    "max_length = 0\n",
    "\n",
    "# Make file for cleaned data\n",
    "cleaned_gpt2_data_train = \"/\".join(input_file_answers.split(\"/\")[:-1]) + \"/cleaned_gpt2_data_train.txt\"\n",
    "cleaned_gpt2_data_eval = \"/\".join(input_file_answers.split(\"/\")[:-1]) + \"/cleaned_gpt2_data_eval.txt\"\n",
    "\n",
    "# Run through every equation in the input files and delete rows with unknown tokens\n",
    "dataset_size = sum(1 for _ in open(input_file_answers, 'rb'))\n",
    "\n",
    "# Open and populate cleaned files and\n",
    "f_cleaned_train = open(cleaned_gpt2_data_train, \"w\")\n",
    "f_cleaned_eval = open(cleaned_gpt2_data_eval, \"w\")\n",
    "\n",
    "start_time = time.time()\n",
    "n_cleaned = 0\n",
    "flag = False\n",
    "for line_number in tqdm(range(1,dataset_size+1)):\n",
    "    # Flag determines whether the data point is an evaluation or a training data point\n",
    "    flag = flag or (line_number-1) % (dataset_size//num_eval_points) == 0\n",
    "    \n",
    "    # Get corresponding equation and expression\n",
    "    raw_equation = linecache.getline(input_file_answers, line_number)\n",
    "    raw_expression = linecache.getline(input_file_expressions, line_number)\n",
    "\n",
    "    # Skip line if aborted error\n",
    "    if raw_equation == \"$Aborted\\n\": continue\n",
    "\n",
    "    # Construct equation and convert to postfix\n",
    "    equation = Equation.makeEquationFromString(raw_equation)\n",
    "    if not equation.tokenized_equation: continue\n",
    "    equation.convertToPostfix()\n",
    "    if equation.notation == \"infix\": continue\n",
    "\n",
    "    # Vectorize corresponding answer (without SOS) and expression (without EOS)\n",
    "    vectorized_answers = vocabulary.vectorize([token.t_type for token in equation.tokenized_equation])[1:]\n",
    "    vectorized_expressions = vocabulary.vectorize([str(token) for token in input_string_to_tokenize_expression(raw_expression)])[:-1]\n",
    "\n",
    "    max_length = max(max_length, len(vectorized_expressions) + len(vectorized_answers) + 1)\n",
    "\n",
    "    if flag:\n",
    "        flag = False\n",
    "        f_cleaned_eval.write(f\"{json.dumps(vectorized_expressions + [vocabulary.getIndex('[SEP]')] + vectorized_answers)}\\n\")\n",
    "    else:\n",
    "        f_cleaned_train.write(f\"{json.dumps(vectorized_expressions + [vocabulary.getIndex('[SEP]')] + vectorized_answers)}\\n\")\n",
    "\n",
    "    # Write them to cleaned data file and separate them by [SEP] token\n",
    "    n_cleaned += 1\n",
    "    \n",
    "f_cleaned_train.close()\n",
    "f_cleaned_eval.close()\n",
    "\n",
    "print(f\"\\nStats: \\n--- Initial size: {dataset_size} \\n--- Time since start: {round(time.time() - start_time, 4)} s \\n--- Successes: {n_cleaned} \\n--- Fails: {line_number - n_cleaned + 1} \\n--- Max length: {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "327e73ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./data/metadata.txt\", \"w\") as f:\n",
    "    f.write(json.dumps({\"max_length\": max_length}))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ae4e53",
   "metadata": {},
   "source": [
    "As per: https://huggingface.co/docs/datasets/create_dataset\n",
    "\n",
    "Make a dataset from an iterator and the push it to the hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "be46a2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336 316 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "336"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_ds_size = sum(1 for _ in open(cleaned_gpt2_data_train, 'rb')) + sum(1 for _ in open(cleaned_gpt2_data_eval, 'rb'))\n",
    "\n",
    "# Sanity check\n",
    "print(processed_ds_size,sum(1 for _ in open(cleaned_gpt2_data_train, 'rb')), sum(1 for _ in open(cleaned_gpt2_data_eval, 'rb')))\n",
    "assert processed_ds_size == n_cleaned\n",
    "\n",
    "processed_ds_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "15bbef5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/data to C:/Users/Hugo/.cache/huggingface/datasets/text/data-60c7b7980a829936/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa4e58ab744747f1a37f15df9eb11ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb6ac8943db244d6b9dcf94b8063f081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to C:/Users/Hugo/.cache/huggingface/datasets/text/data-60c7b7980a829936/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b17251887c49fd938d6ec8ce8eefc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 316\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"/\".join(cleaned_gpt2_data_train.split(\"/\")[:-1]), \n",
    "             data_files={\"train\": cleaned_gpt2_data_train.split(\"/\")[-1], \"validation\": cleaned_gpt2_data_eval.split(\"/\")[-1]})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "08357d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/datasets/Dragonoverlord3000/JustSumAI_cleaned_gpt2_data', endpoint='https://huggingface.co', repo_type='dataset', repo_id='Dragonoverlord3000/JustSumAI_cleaned_gpt2_data')"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_repo(repo_name, exist_ok=True, repo_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5aea7dd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split train to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "759dedbba9f341cc837fbd9a40e41e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5df59382a254768a12a6268c2890114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9497e4eee3204de7a17858705a4c5b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef6aab06ee546c3bed31b9863f6610e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split validation to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9186f5596dc409eb9c73b668003cb5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91cedefb5f144c69877ee63e5c222bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3d640baa9d48429caf7f9d10ea1016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.push_to_hub(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b521f06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to C:/Users/Hugo/.cache/huggingface/datasets/Dragonoverlord3000___parquet/Dragonoverlord3000--JustSumAI_cleaned_gpt2_data-7bb197e8e347c6a0/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "041db9c0f7274f62a772399f46139ea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9a8aad67774d588fa09b15a5a9a424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/49.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58aeae411e61468c84e8e0ef54108a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/336 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to C:/Users/Hugo/.cache/huggingface/datasets/Dragonoverlord3000___parquet/Dragonoverlord3000--JustSumAI_cleaned_gpt2_data-7bb197e8e347c6a0/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d576ffe32b047d98c855d554733df77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(f\"Dragonoverlord3000/{repo_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6a914aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[64,\n",
       " 9,\n",
       " 39,\n",
       " 25,\n",
       " 22,\n",
       " 21,\n",
       " 0,\n",
       " 4,\n",
       " 26,\n",
       " 23,\n",
       " 67,\n",
       " 51,\n",
       " 49,\n",
       " 49,\n",
       " 49,\n",
       " 61,\n",
       " 48,\n",
       " 59,\n",
       " 49,\n",
       " 61,\n",
       " 51,\n",
       " 49,\n",
       " 49,\n",
       " 61,\n",
       " 59,\n",
       " 48,\n",
       " 58,\n",
       " 49,\n",
       " 61,\n",
       " 58,\n",
       " 49,\n",
       " 48,\n",
       " 60,\n",
       " 49,\n",
       " 61,\n",
       " 58,\n",
       " 49,\n",
       " 44,\n",
       " 60,\n",
       " 49,\n",
       " 61,\n",
       " 58,\n",
       " 49,\n",
       " 49,\n",
       " 53,\n",
       " 60,\n",
       " 44,\n",
       " 60,\n",
       " 49,\n",
       " 61,\n",
       " 59,\n",
       " 49,\n",
       " 49,\n",
       " 49,\n",
       " 49,\n",
       " 61,\n",
       " 49,\n",
       " 53,\n",
       " 49,\n",
       " 61,\n",
       " 58,\n",
       " 61,\n",
       " 53,\n",
       " 60,\n",
       " 44,\n",
       " 60,\n",
       " 49,\n",
       " 61,\n",
       " 58,\n",
       " 49,\n",
       " 44,\n",
       " 60,\n",
       " 49,\n",
       " 49,\n",
       " 49,\n",
       " 61,\n",
       " 49,\n",
       " 53,\n",
       " 49,\n",
       " 61,\n",
       " 58,\n",
       " 53,\n",
       " 60,\n",
       " 61,\n",
       " 59,\n",
       " 49,\n",
       " 49,\n",
       " 57,\n",
       " 60,\n",
       " 49,\n",
       " 61,\n",
       " 59,\n",
       " 49,\n",
       " 49,\n",
       " 57,\n",
       " 60,\n",
       " 49,\n",
       " 61,\n",
       " 58,\n",
       " 49,\n",
       " 49,\n",
       " 57,\n",
       " 60,\n",
       " 49,\n",
       " 61,\n",
       " 59,\n",
       " 49,\n",
       " 49,\n",
       " 49,\n",
       " 53,\n",
       " 59,\n",
       " 49,\n",
       " 61,\n",
       " 57,\n",
       " 60,\n",
       " 49,\n",
       " 61,\n",
       " 59,\n",
       " 49,\n",
       " 49,\n",
       " 53,\n",
       " 60,\n",
       " 49,\n",
       " 49,\n",
       " 53,\n",
       " 59,\n",
       " 49,\n",
       " 61,\n",
       " 57,\n",
       " 60,\n",
       " 49,\n",
       " 61,\n",
       " 59,\n",
       " 49,\n",
       " 49,\n",
       " 49,\n",
       " 53,\n",
       " 58,\n",
       " 49,\n",
       " 61,\n",
       " 57,\n",
       " 60,\n",
       " 49,\n",
       " 61,\n",
       " 59,\n",
       " 49,\n",
       " 49,\n",
       " 53,\n",
       " 60,\n",
       " 49,\n",
       " 49,\n",
       " 53,\n",
       " 58,\n",
       " 49,\n",
       " 61,\n",
       " 57,\n",
       " 60,\n",
       " 49,\n",
       " 61,\n",
       " 58,\n",
       " 60,\n",
       " 59,\n",
       " 65]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(dataset[\"train\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ea26d2",
   "metadata": {},
   "source": [
    "# Training GPT-2 from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651863c0",
   "metadata": {},
   "source": [
    "### Loading the model\n",
    "The model is GPT-2 with 1.5 BILLION  parameters!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "87f62352",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(68, 1600)\n",
       "    (wpe): Embedding(1024, 1600)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-47): 48 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1600, out_features=68, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\"gpt2-xl\", vocab_size=len(vocabulary))\n",
    "model = AutoModelForCausalLM.from_config(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "52dd0187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPT-2 (xl) size: 1477.3M parameters'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"GPT-2 (xl) size: {model.num_parameters()/1_000_000:.1f}M parameters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f580704c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ebc5d6f50e64ecb888c29c1d0543aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be13044d658f4aee8952a1514c8bf764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/5.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save newly initialized model - takes a while\n",
    "model.save_pretrained(f\"models/{model_name}\", \n",
    "                      push_to_hub=True, \n",
    "                      organization=\"Dragonoverlord3000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf7e17e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cc43c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3e0540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e0f58a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2ec17a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582fedf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18a624b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
