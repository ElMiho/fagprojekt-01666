{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "90888fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External imports\n",
    "import torch\n",
    "\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "import datasets\n",
    "import transformers\n",
    "from transformers import AutoModel, set_seed, get_scheduler, AutoModelForCausalLM\n",
    "from huggingface_hub import Repository, create_repo\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from argparse import Namespace\n",
    "import logging\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b8c9d575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domestic imports\n",
    "from model.tokenize_input import input_string_to_tokenize_expression\n",
    "from model.tokens import TOKEN_TYPE_ANSWERS, TOKEN_TYPE_EXPRESSIONS\n",
    "from model.equation_interpreter import Equation\n",
    "from model.vocabulary import Vocabulary\n",
    "from model.tokens import Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a277156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined vocabulary\n",
    "vocabulary = Vocabulary.construct_from_list(TOKEN_TYPE_EXPRESSIONS + TOKEN_TYPE_ANSWERS)\n",
    "vectorized_sample = vocabulary.vectorize([\"#\", \"/\", \"0\", \"-1\", \"[SEP]\", \"TT_INTEGER\"])\n",
    "vectorized_sample, [vocabulary.getToken(idx) for idx in vectorized_sample]\n",
    "\n",
    "# Global variables\n",
    "model_name = \"JustSumAI\"\n",
    "project_name = \"JustSumAI\"\n",
    "repo_name = f\"{model_name}_cleaned_gpt2_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bf3cd7",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a9a9579",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/metadata.txt\", \"r\") as f:\n",
    "    max_length = json.loads(f.read())[\"max_length\"]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5cad31d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"train_batch_size\": 8,\n",
    "    \"valid_batch_size\": 8,\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"shuffle_buffer\": 1000,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"num_warmup_steps\": 750,\n",
    "    \"gradient_accumulation_steps\": 16,\n",
    "    \"max_train_steps\": 50_000,\n",
    "    \"max_eval_steps\": -1,\n",
    "    \"seq_length\": max_length,\n",
    "    \"seed\": 628,\n",
    "    \"save_checkpoint_steps\": 50_000,\n",
    "    \"model_name\": model_name\n",
    "}\n",
    "args = Namespace(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "56eace89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/datasets/Dragonoverlord3000/Dragonoverlord3000', endpoint='https://huggingface.co', repo_type='dataset', repo_id='Dragonoverlord3000/Dragonoverlord3000')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_repo(args.save_dir, exist_ok=True, repo_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f8215a",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "948a530f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = args.train_batch_size\n",
    "BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75660fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/Hugo/.cache/huggingface/datasets/Dragonoverlord3000___parquet/Dragonoverlord3000--JustSumAI_cleaned_gpt2_data-5a0747bdc3dcbe7d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d7ef9545374883bfd3555f4110a815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 316\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(f\"Dragonoverlord3000/{repo_name}\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "50fbd303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 28, 39, 14, 30, 15, 0, 5, 26, 67, 51, 49, 51, 49, 49, 61, 59, 49, 48, 60, 49, 61, 58, 51, 49, 49, 61, 59, 49, 49, 49, 61, 48, 59, 60, 58, 48, 58, 49, 61, 58, 49, 44, 60, 49, 61, 58, 49, 49, 53, 60, 44, 60, 49, 61, 59, 49, 49, 49, 49, 61, 49, 53, 49, 61, 58, 61, 53, 60, 44, 60, 49, 61, 58, 49, 44, 60, 49, 49, 49, 61, 49, 53, 49, 61, 58, 53, 60, 61, 59, 49, 49, 57, 60, 49, 61, 59, 49, 49, 57, 60, 49, 61, 58, 49, 49, 57, 60, 49, 61, 58, 49, 49, 49, 53, 59, 49, 61, 57, 60, 49, 61, 58, 49, 49, 53, 60, 49, 49, 53, 59, 49, 61, 57, 60, 49, 61, 58, 49, 49, 49, 53, 58, 49, 61, 57, 60, 49, 61, 58, 49, 49, 53, 60, 49, 49, 53, 58, 49, 61, 57, 60, 49, 61, 59, 60, 59, 65]\n",
      "['<BEGIN>', '4/5', '/', '-1/2', '5/4', '-2/5', '-5', '-5/3', '2/3', '[SEP]', 'TT_ZERO', 'TT_INTEGER', 'TT_ZERO', 'TT_INTEGER', 'TT_INTEGER', 'TT_DIVIDE', 'TT_MINUS', 'TT_INTEGER', 'TT_EULERGAMMA', 'TT_MULTIPLY', 'TT_INTEGER', 'TT_DIVIDE', 'TT_PLUS', 'TT_ZERO', 'TT_INTEGER', 'TT_INTEGER', 'TT_DIVIDE', 'TT_MINUS', 'TT_INTEGER', 'TT_INTEGER', 'TT_INTEGER', 'TT_DIVIDE', 'TT_EULERGAMMA', 'TT_MINUS', 'TT_MULTIPLY', 'TT_PLUS', 'TT_EULERGAMMA', 'TT_PLUS', 'TT_INTEGER', 'TT_DIVIDE', 'TT_PLUS', 'TT_INTEGER', 'TT_PI', 'TT_MULTIPLY', 'TT_INTEGER', 'TT_DIVIDE', 'TT_PLUS', 'TT_INTEGER', 'TT_INTEGER', 'TT_SQRT', 'TT_MULTIPLY', 'TT_PI', 'TT_MULTIPLY', 'TT_INTEGER', 'TT_DIVIDE', 'TT_MINUS', 'TT_INTEGER', 'TT_INTEGER', 'TT_INTEGER', 'TT_INTEGER', 'TT_DIVIDE', 'TT_INTEGER', 'TT_SQRT', 'TT_INTEGER', 'TT_DIVIDE', 'TT_PLUS', 'TT_DIVIDE', 'TT_SQRT', 'TT_MULTIPLY', 'TT_PI', 'TT_MULTIPLY', 'TT_INTEGER', 'TT_DIVIDE', 'TT_PLUS', 'TT_INTEGER', 'TT_PI', 'TT_MULTIPLY', 'TT_INTEGER', 'TT_INTEGER', 'TT_INTEGER', 'TT_DIVIDE', 'TT_INTEGER', 'TT_SQRT', 'TT_INTEGER', 'TT_DIVIDE', 'TT_PLUS', 'TT_SQRT', 'TT_MULTIPLY', 'TT_DIVIDE', 'TT_MINUS', 'TT_INTEGER', 'TT_INTEGER', 'TT_LOG', 'TT_MULTIPLY', 'TT_INTEGER', 'TT_DIVIDE', 'TT_MINUS', 'TT_INTEGER', 'TT_INTEGER', 'TT_LOG', 'TT_MULTIPLY', 'TT_INTEGER', 'TT_DIVIDE', 'TT_PLUS', 'TT_INTEGER', 'TT_INTEGER', 'TT_LOG', 'TT_MULTIPLY', 'TT_INTEGER', 'TT_DIVIDE', 'TT_PLUS', 'TT_INTEGER', 'TT_INTEGER', 'TT_INTEGER', 'TT_SQRT', 'TT_MINUS', 'TT_INTEGER', 'TT_DIVIDE', 'TT_LOG', 'TT_MULTIPLY', 'TT_INTEGER', 'TT_DIVIDE', 'TT_PLUS', 'TT_INTEGER', 'TT_INTEGER', 'TT_SQRT', 'TT_MULTIPLY', 'TT_INTEGER', 'TT_INTEGER', 'TT_SQRT', 'TT_MINUS', 'TT_INTEGER', 'TT_DIVIDE', 'TT_LOG', 'TT_MULTIPLY', 'TT_INTEGER', 'TT_DIVIDE', 'TT_PLUS', 'TT_INTEGER', 'TT_INTEGER', 'TT_INTEGER', 'TT_SQRT', 'TT_PLUS', 'TT_INTEGER', 'TT_DIVIDE', 'TT_LOG', 'TT_MULTIPLY', 'TT_INTEGER', 'TT_DIVIDE', 'TT_PLUS', 'TT_INTEGER', 'TT_INTEGER', 'TT_SQRT', 'TT_MULTIPLY', 'TT_INTEGER', 'TT_INTEGER', 'TT_SQRT', 'TT_PLUS', 'TT_INTEGER', 'TT_DIVIDE', 'TT_LOG', 'TT_MULTIPLY', 'TT_INTEGER', 'TT_DIVIDE', 'TT_MINUS', 'TT_MULTIPLY', 'TT_MINUS', '<END>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'(-(Z*((((((((((((((-(Z/Z))+((Z*EulerGamma)/Z))+((((-(Z/Z))+(Z*((Z/Z)-EulerGamma)))+EulerGamma)/Z))+((Z*Pi)/Z))-(((Z*Sqrt(Z))*Pi)/Z))+(((Z*Sqrt((Z/((Z/Z)+(Sqrt(Z)/Z)))))*Pi)/Z))-((Z*Pi)/(Z*Sqrt(((Z/Z)+(Sqrt(Z)/Z))))))-((Z*Log(Z))/Z))+((Z*Log(Z))/Z))+((Z*Log(Z))/Z))+((Z*Log(((Z-Sqrt(Z))/Z)))/Z))+(((Z*Sqrt(Z))*Log(((Z-Sqrt(Z))/Z)))/Z))+((Z*Log(((Z+Sqrt(Z))/Z)))/Z))-(((Z*Sqrt(Z))*Log(((Z+Sqrt(Z))/Z)))/Z))))'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print sample data\n",
    "print(json.loads(dataset[\"train\"][0][\"text\"]))\n",
    "l = [vocabulary.getToken(idx) for idx in json.loads(dataset[\"train\"][0][\"text\"])]\n",
    "print(l)\n",
    "eq = Equation([Token(t) for t in l[l.index(\"[SEP]\")+1:-1]], notation=\"postfix\")\n",
    "eq.getMathmetaicalNotation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "27e25cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x167a7844280>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x167a7844880>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader = DataLoader(dataset[\"train\"], shuffle=True, batch_size=BATCH_SIZE)\n",
    "eval_dataloader = DataLoader(dataset[\"validation\"], shuffle=True, batch_size=BATCH_SIZE)\n",
    "train_dataloader, eval_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d909b1d",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6c4b5eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file models/JustSumAI\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"models/JustSumAI\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1600,\n",
      "  \"n_head\": 25,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 48,\n",
      "  \"n_positions\": 1024,\n",
      "  \"output_past\": true,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.27.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 68\n",
      "}\n",
      "\n",
      "loading weights file models/JustSumAI\\pytorch_model.bin\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "GPT2LMHeadModel.__init__() got an unexpected keyword argument 'organization'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [78], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morganization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDragonoverlord3000\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m model\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:471\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    470\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    472\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    473\u001b[0m     )\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    477\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:2498\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2495\u001b[0m     init_contexts\u001b[38;5;241m.\u001b[39mappend(init_empty_weights())\n\u001b[0;32m   2497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m-> 2498\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(config, \u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2500\u001b[0m \u001b[38;5;66;03m# Check first if we are `from_pt`\u001b[39;00m\n\u001b[0;32m   2501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_keep_in_fp32_modules:\n",
      "\u001b[1;31mTypeError\u001b[0m: GPT2LMHeadModel.__init__() got an unexpected keyword argument 'organization'"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(f\"models/{model_name}\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed5501e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPT-2 Number of parameters: 1477.31M'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"GPT-2 Number of parameters: {model.num_parameters()/1_000_000:.2f}M\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2664fc",
   "metadata": {},
   "source": [
    "### Define weight decay parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a434d2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n,p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "            \n",
    "        return [{\"params\": params_with_wd, \"weight_decay\": args.weight_decay},\n",
    "               {\"params\": params_without_wd, \"weight_decay\": 0.0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9eda92",
   "metadata": {},
   "source": [
    "### Setting up logging for the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c138b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step,batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch, labels=batch)\n",
    "        loss = outputs.loss.repeat(args.valid_batch_size)\n",
    "        losses.append(accelerator.gather(loss))\n",
    "        if args.max_eval_steps > 0 and step >= args.max_eval_steps: break\n",
    "    loss = torch.mean(torch.cat(losses))\n",
    "    # Lower perplexity implies better performance\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = torch.tensor(float(\"inf\"))\n",
    "    return loss.item(), perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2f65e42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone model repository\n",
    "if accelerator.is_main_process:\n",
    "    hf_repo = Repository(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b1da6d",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa745c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for model\n",
    "set_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9fca76ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accelerator\n",
    "accelerator = Accelerator()\n",
    "samples_per_step = accelerator.state.num_processes * args.train_batch_size\n",
    "samples_per_step, accelerator.is_main_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cedf93d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Distributed environment: NO\n",
       "Num processes: 1\n",
       "Process index: 0\n",
       "Local process index: 0\n",
       "Device: cpu\n",
       "\n",
       "Mixed precision type: no"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accelerator.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14819e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the optimizer and learning rate scheduler\n",
    "optimizer = AdamW(get_grouped_params(model), lr=args.learning_rate)\n",
    "lr_scheduler = get_scheduler(name=args.lr_scheduler_type, optimizer=optimizer,\n",
    "                            num_warmup_steps=args.num_warmup_steps, \n",
    "                            num_training_steps=args.max_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ebd8b27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr():\n",
    "    return optimizer.param_groups[0][\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa0c629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare everything  with our `accelerator` (order of args is not important)\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fe439fbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GPT2Model.forward() got an unexpected keyword argument 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [73], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m completed_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m      6\u001b[0m     loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n\u001b[0;32m      7\u001b[0m     accelerator\u001b[38;5;241m.\u001b[39mbackward(loss)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[1;31mTypeError\u001b[0m: GPT2Model.forward() got an unexpected keyword argument 'labels'"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "model.train()\n",
    "completed_steps = 0\n",
    "for step, batch in enumerate(train_dataloader, start=1):\n",
    "    loss = model(batch, labels=batch).loss\n",
    "    loss /= args.gradient_accumulation_steps\n",
    "    accelerator.backward(loss)\n",
    "    if step % args.gradient_accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        completed_steps += 1\n",
    "    if step % args.save_checkpoint_steps == 0:\n",
    "        logger.info(\"Evaluating and saving model checkpoint\")\n",
    "        eval_loss, perplexity = evaluate()\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        if accelerator.is_main_process:\n",
    "            model.save_pretrained(f\"models/{model_name}\", \n",
    "                      push_to_hub=True, \n",
    "                      organization=\"Dragonoverlord3000\")\n",
    "        if completed_steps >= args.max_train_steps:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6ca45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and save the last checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b90ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc516b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0ee852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed474fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dad4b12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216796ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cac982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
