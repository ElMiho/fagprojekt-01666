{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90888fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External imports\n",
    "import torch\n",
    "\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "import datasets\n",
    "import transformers\n",
    "from transformers import AutoModel, set_seed, get_scheduler, AutoModelForCausalLM\n",
    "from huggingface_hub import Repository, create_repo\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from argparse import Namespace\n",
    "import logging\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8c9d575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domestic imports\n",
    "from model.tokenize_input import input_string_to_tokenize_expression\n",
    "from model.tokens import TOKEN_TYPE_ANSWERS, TOKEN_TYPE_EXPRESSIONS\n",
    "from model.equation_interpreter import Equation\n",
    "from model.vocabulary import Vocabulary\n",
    "from model.tokens import Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a277156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined vocabulary\n",
    "vocabulary = Vocabulary.construct_from_list(TOKEN_TYPE_EXPRESSIONS + TOKEN_TYPE_ANSWERS)\n",
    "vectorized_sample = vocabulary.vectorize([\"#\", \"/\", \"0\", \"-1\", \"[SEP]\", \"TT_INTEGER\"])\n",
    "vectorized_sample, [vocabulary.getToken(idx) for idx in vectorized_sample]\n",
    "\n",
    "# Global variables\n",
    "model_name = \"JustSumAI\"\n",
    "project_name = \"JustSumAI\"\n",
    "repo_name = f\"{model_name}_cleaned_gpt2_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bf3cd7",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a9a9579",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/metadata.txt\", \"r\") as f:\n",
    "    max_length = json.loads(f.read())[\"max_length\"]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cad31d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"train_batch_size\": 8,\n",
    "    \"valid_batch_size\": 8,\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"shuffle_buffer\": 1000,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"num_warmup_steps\": 750,\n",
    "    \"gradient_accumulation_steps\": 16,\n",
    "    \"max_train_steps\": 50_000,\n",
    "    \"max_eval_steps\": -1,\n",
    "    \"seq_length\": max_length,\n",
    "    \"seed\": 628,\n",
    "    \"save_checkpoint_steps\": 50_000,\n",
    "    \"save_dir\": \"./models/JustSumAI\",\n",
    "    \"model_name\": model_name\n",
    "}\n",
    "args = Namespace(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f8215a",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "948a530f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = args.train_batch_size\n",
    "BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75660fd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/Hugo/.cache/huggingface/datasets/Dragonoverlord3000___parquet/Dragonoverlord3000--JustSumAI_cleaned_gpt2_data-9dc6473027dd9156/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca667e3e71b64624b50610ead89e17b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 316\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(f\"Dragonoverlord3000/{repo_name}\")#, streaming=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ff52ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(dataset[\"train\"]):\n",
    "    if i > 0: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50fbd303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 10, 5, 31, 14, 31, 6, 39, 4, 10, 35, 18, 1, 33, 11, 16, 21, 67, 51, 49, 51, 49, 49, 61, 59, 49, 49, 49, 61, 48, 59, 60, 49, 61, 59, 49, 49, 49, 61, 48, 59, 60, 49, 61, 58, 49, 48, 60, 49, 61, 58, 49, 44, 60, 49, 61, 58, 49, 49, 53, 60, 44, 60, 49, 61, 59, 49, 49, 49, 49, 61, 49, 53, 49, 61, 59, 61, 53, 60, 44, 60, 49, 61, 58, 49, 44, 60, 49, 49, 49, 61, 49, 53, 49, 61, 59, 53, 60, 61, 58, 49, 49, 57, 60, 49, 61, 59, 49, 49, 57, 60, 49, 61, 59, 49, 49, 57, 60, 49, 61, 58, 49, 49, 49, 53, 59, 49, 61, 57, 60, 49, 61, 58, 49, 49, 53, 60, 49, 49, 53, 59, 49, 61, 57, 60, 49, 61, 59, 49, 49, 49, 53, 58, 49, 61, 57, 60, 49, 61, 58, 49, 49, 53, 60, 49, 49, 53, 58, 49, 61, 57, 60, 49, 61, 58, 60, 59, 49, 61, 65]\n",
      "['<BEGIN>', '-4/5', '-5/3', '4/3', '-1/2', '4/3', '-3/2', '/', '-2', '-4/5', '5/2', '-1/5', '-4', '5/3', '-3/4', '-1/3', '1/4', '[SEP]', 'TT_ZERO', 'TT_INTEGER', 'TT_ZERO', 'TT_INTEGER', 'TT_INTEGER', 'TT_DIVIDE', 'TT_MINUS', 'TT_INTEGER', 'TT_INTEGER', 'TT_INTEGER', 'TT_DIVIDE', 'TT_EULERGAMMA', 'TT_MINUS', 'TT_MULTIPLY', 'TT_INTEGER', 'TT_DIVIDE', 'TT_MINUS', 'TT_INTEGER', 'TT_INTEGER', 'TT_INTEGER', 'TT_DIVIDE', 'TT_EULERGAMMA', 'TT_MINUS', 'TT_MULTIPLY', 'TT_INTEGER', 'TT_DIVIDE', 'TT_PLUS', 'TT_INTEGER', 'TT_EULERGAMMA', 'TT_MULTIPLY', 'TT_INTEGER', 'TT_DIVIDE', 'TT_PLUS', 'TT_INTEGER', 'TT_PI', 'TT_MULTIPLY', 'TT_INTEGER', 'TT_DIVIDE', 'TT_PLUS', 'TT_INTEGER', 'TT_INTEGER', 'TT_SQRT', 'TT_MULTIPLY', 'TT_PI', 'TT_MULTIPLY', 'TT_INTEGER', 'TT_DIVIDE', 'TT_MINUS', 'TT_INTEGER', 'TT_INTEGER', 'TT_INTEGER', 'TT_INTEGER', 'TT_DIVIDE', 'TT_INTEGER', 'TT_SQRT', 'TT_INTEGER', 'TT_DIVIDE', 'TT_MINUS', 'TT_DIVIDE', 'TT_SQRT', 'TT_MULTIPLY', 'TT_PI', 'TT_MULTIPLY', 'TT_INTEGER', 'TT_DIVIDE', 'TT_PLUS', 'TT_INTEGER', 'TT_PI', 'TT_MULTIPLY', 'TT_INTEGER', 'TT_INTEGER', 'TT_INTEGER', 'TT_DIVIDE', 'TT_INTEGER', 'TT_SQRT', 'TT_INTEGER', 'TT_DIVIDE', 'TT_MINUS', 'TT_SQRT', 'TT_MULTIPLY', 'TT_DIVIDE', 'TT_PLUS', 'TT_INTEGER', 'TT_INTEGER', 'TT_LOG', 'TT_MULTIPLY', 'TT_INTEGER', 'TT_DIVIDE', 'TT_MINUS', 'TT_INTEGER', 'TT_INTEGER', 'TT_LOG', 'TT_MULTIPLY', 'TT_INTEGER', 'TT_DIVIDE', 'TT_MINUS', 'TT_INTEGER', 'TT_INTEGER', 'TT_LOG', 'TT_MULTIPLY', 'TT_INTEGER', 'TT_DIVIDE', 'TT_PLUS', 'TT_INTEGER', 'TT_INTEGER', 'TT_INTEGER', 'TT_SQRT', 'TT_MINUS', 'TT_INTEGER', 'TT_DIVIDE', 'TT_LOG', 'TT_MULTIPLY', 'TT_INTEGER', 'TT_DIVIDE', 'TT_PLUS', 'TT_INTEGER', 'TT_INTEGER', 'TT_SQRT', 'TT_MULTIPLY', 'TT_INTEGER', 'TT_INTEGER', 'TT_SQRT', 'TT_MINUS', 'TT_INTEGER', 'TT_DIVIDE', 'TT_LOG', 'TT_MULTIPLY', 'TT_INTEGER', 'TT_DIVIDE', 'TT_MINUS', 'TT_INTEGER', 'TT_INTEGER', 'TT_INTEGER', 'TT_SQRT', 'TT_PLUS', 'TT_INTEGER', 'TT_DIVIDE', 'TT_LOG', 'TT_MULTIPLY', 'TT_INTEGER', 'TT_DIVIDE', 'TT_PLUS', 'TT_INTEGER', 'TT_INTEGER', 'TT_SQRT', 'TT_MULTIPLY', 'TT_INTEGER', 'TT_INTEGER', 'TT_SQRT', 'TT_PLUS', 'TT_INTEGER', 'TT_DIVIDE', 'TT_LOG', 'TT_MULTIPLY', 'TT_INTEGER', 'TT_DIVIDE', 'TT_PLUS', 'TT_MULTIPLY', 'TT_MINUS', 'TT_INTEGER', 'TT_DIVIDE', '<END>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'((-(Z*(((((((((((((((-(Z/Z))-((Z*((Z/Z)-EulerGamma))/Z))+((Z*((Z/Z)-EulerGamma))/Z))+((Z*EulerGamma)/Z))+((Z*Pi)/Z))-(((Z*Sqrt(Z))*Pi)/Z))+(((Z*Sqrt((Z/((Z/Z)-(Sqrt(Z)/Z)))))*Pi)/Z))+((Z*Pi)/(Z*Sqrt(((Z/Z)-(Sqrt(Z)/Z))))))-((Z*Log(Z))/Z))-((Z*Log(Z))/Z))+((Z*Log(Z))/Z))+((Z*Log(((Z-Sqrt(Z))/Z)))/Z))-(((Z*Sqrt(Z))*Log(((Z-Sqrt(Z))/Z)))/Z))+((Z*Log(((Z+Sqrt(Z))/Z)))/Z))+(((Z*Sqrt(Z))*Log(((Z+Sqrt(Z))/Z)))/Z))))/Z)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print sample data\n",
    "print(json.loads(data[\"text\"]))\n",
    "l = [vocabulary.getToken(idx) for idx in json.loads(data[\"text\"])]\n",
    "print(l)\n",
    "eq = Equation([Token(t) for t in l[l.index(\"[SEP]\")+1:-1]], notation=\"postfix\")\n",
    "eq.getMathmetaicalNotation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27e25cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x1a79fa2b7f0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x1a79fa2b4c0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader = DataLoader(dataset[\"train\"], shuffle=True, batch_size=BATCH_SIZE)\n",
    "eval_dataloader = DataLoader(dataset[\"validation\"], shuffle=True, batch_size=BATCH_SIZE)\n",
    "train_dataloader, eval_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fa30979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processer(batch):\n",
    "    output = []\n",
    "    for element in batch[\"text\"]:\n",
    "        element = json.loads(element)\n",
    "        # Make sure to pad output to max length\n",
    "        output.append(\n",
    "            element + [vocabulary.mask_index] * (args.seq_length - len(element))\n",
    "        )\n",
    "        \n",
    "    output = torch.LongTensor(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2f88bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[64, 10,  0,  ..., 63, 63, 63],\n",
       "         [64, 20,  1,  ..., 63, 63, 63],\n",
       "         [64, 40, 39,  ..., 63, 63, 63],\n",
       "         ...,\n",
       "         [64, 11, 39,  ..., 63, 63, 63],\n",
       "         [64, 22,  7,  ..., 63, 63, 63],\n",
       "         [64, 40, 39,  ..., 63, 63, 63]]),\n",
       " tensor([64, 10,  0, 16, 19, 39, 35,  6, 19, 17, 10,  2,  4,  7, 67, 51, 49, 51,\n",
       "         49, 59, 49, 44, 60, 58, 49, 49, 53, 60, 44, 60, 58, 49, 49, 57, 60, 59,\n",
       "         49, 49, 57, 60, 58, 60, 59, 49, 61, 65, 63, 63, 63, 63, 63, 63, 63, 63,\n",
       "         63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63,\n",
       "         63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63,\n",
       "         63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63,\n",
       "         63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63,\n",
       "         63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63,\n",
       "         63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63,\n",
       "         63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63,\n",
       "         63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,data_ in enumerate(train_dataloader):\n",
    "    if i > 0:\n",
    "        break\n",
    "    data = data_processer(data_)\n",
    "    \n",
    "data, data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d909b1d",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c4b5eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(68, 1600)\n",
       "    (wpe): Embedding(1024, 1600)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-47): 48 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1600, out_features=68, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(f\"models/{model_name}\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed5501e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPT-2 Number of parameters: 1477.31M'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"GPT-2 Number of parameters: {model.num_parameters()/1_000_000:.2f}M\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2664fc",
   "metadata": {},
   "source": [
    "### Define weight decay parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a434d2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n,p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "            \n",
    "        return [{\"params\": params_with_wd, \"weight_decay\": args.weight_decay},\n",
    "               {\"params\": params_without_wd, \"weight_decay\": 0.0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9eda92",
   "metadata": {},
   "source": [
    "### Setting up logging for the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c138b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step,batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch, labels=batch)\n",
    "        loss = outputs.loss.repeat(args.valid_batch_size)\n",
    "        losses.append(accelerator.gather(loss))\n",
    "        if args.max_eval_steps > 0 and step >= args.max_eval_steps: break\n",
    "    loss = torch.mean(torch.cat(losses))\n",
    "    # Lower perplexity implies better performance\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = torch.tensor(float(\"inf\"))\n",
    "    return loss.item(), perplexity.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b1da6d",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa745c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for model\n",
    "set_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fca76ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accelerator\n",
    "accelerator = Accelerator()\n",
    "samples_per_step = accelerator.state.num_processes * args.train_batch_size\n",
    "samples_per_step, accelerator.is_main_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f65e42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone model repository\n",
    "if accelerator.is_main_process:\n",
    "    hf_repo = Repository(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cedf93d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Distributed environment: NO\n",
       "Num processes: 1\n",
       "Process index: 0\n",
       "Local process index: 0\n",
       "Device: cpu\n",
       "\n",
       "Mixed precision type: no"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accelerator.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14819e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the optimizer and learning rate scheduler\n",
    "optimizer = AdamW(get_grouped_params(model), lr=args.learning_rate)\n",
    "lr_scheduler = get_scheduler(name=args.lr_scheduler_type, optimizer=optimizer,\n",
    "                            num_warmup_steps=args.num_warmup_steps, \n",
    "                            num_training_steps=args.max_train_steps)\n",
    "\n",
    "accelerator.register_for_checkpointing(lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebd8b27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr():\n",
    "    return optimizer.param_groups[0][\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa0c629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare everything  with our `accelerator` (order of args is not important)\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa43cbd",
   "metadata": {},
   "source": [
    "### Training Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe439fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model.train()\n",
    "completed_steps = 0\n",
    "for step, batch in enumerate(train_dataloader, start=1):\n",
    "    batch = data_processer(batch)\n",
    "    loss = model(batch, labels=batch).loss\n",
    "    loss /= args.gradient_accumulation_steps\n",
    "    accelerator.backward(loss)\n",
    "    if step % args.gradient_accumulation_steps == 0:\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        completed_steps += 1\n",
    "        \n",
    "    if step % args.save_checkpoint_steps == 0:\n",
    "        logger.info(\"Evaluating and saving model checkpoint\")\n",
    "        eval_loss, perplexity = evaluate()\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        if accelerator.is_main_process:\n",
    "            model.save_pretrained(f\"models/{model_name}\", \n",
    "                      push_to_hub=True, \n",
    "                      organization=\"Dragonoverlord3000\")\n",
    "        if completed_steps >= args.max_train_steps:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6ca45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and save the last checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b90ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc516b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0ee852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed474fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dad4b12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216796ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cac982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
